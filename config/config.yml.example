# Example Configuration for AI RAG Chat
#
# This file centralizes the configuration for LLM providers and API keys.
# You can copy this to config.yml and modify it with your specific settings.

# Default model selections
# These values will be used as the default selections in the UI and Cli
defaults:
  chat_model_provider: ollama
  chat_model: llama3
  embedding_model_provider: ollama
  embedding_model: nomic-embed-text:latest

# LLM Provider Configurations
# Define the base URLs for your LLM services.
providers:
  ollama:
    url: http://localhost:11434
  lm_studio:
    url: http://localhost:1234
  litellm:
    url: http://localhost:4000
  openrouter:
    url: https://openrouter.ai/api

# API Keys for various LLM providers
# These keys will be used by LiteLLM to authenticate with the respective LLM services.
# The system will intelligently select the correct key based on the model being used.
api_keys:
  # LiteLLM proxy API key (if your LiteLLM proxy requires authentication)
  litellm: your_litellm_proxy_api_key_if_any # Optional
  # OpenAI API Key (e.g., for GPT models via LiteLLM)
  openai: sk-your_openai_api_key
  # Anthropic API Key (e.g., for Claude models via LiteLLM)
  anthropic: sk-your_anthropic_api_key
  # OpenRouter API Key (for accessing OpenRouter models)
  openrouter: sk-or-your_openrouter_api_key
  # Add other provider API keys as needed (e.g., cohere, google, etc.)
  # cohere: your_cohere_api_key
  # google: your_google_api_key

# Whitelist for embedding models. This helps to distinguish between chat and embedding models.
embedding_models_whitelist:
  - nomic-embed-text:latest
  - text-embedding-nomic-embed-text-v1.5

# Document Ingestion Configuration
# List of paths to documents you want to chat with
# Comment out or remove this section to use the system in chatbot-only mode
ingest_docs:
  - documents/sample1.pdf
  - documents/sample2.md
  - documents/folder/**/*.txt  # Supports glob patterns
